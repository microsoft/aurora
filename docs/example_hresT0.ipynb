{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2f57a10-13a1-4f66-a734-065fc16b17b2",
   "metadata": {},
   "source": [
    "# Full Example: Predictions for HRES T0\n",
    "\n",
    "In this example, we will download HRES T0 data for 11 May 2022 from WeatherBench2 at 0.25 degrees resolution and run Aurora on this data. We will use the version of Aurora that was fine-tuned on IFS HRES T0 in this example. \n",
    "\n",
    "Running this notebook requires installing an additional python package in order to download from Google Cloud Storage:\n",
    "\n",
    "`pip install gcsfs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75168b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface-level variables downloaded!\n",
      "Atmos-level variables downloaded!\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import fsspec\n",
    "import gcsfs\n",
    "\n",
    "# Data will be downloaded here.\n",
    "download_path = Path(\"~/downloads\")\n",
    "download_path = download_path.expanduser()\n",
    "download_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load outline of Zarr archive. \n",
    "url = \"gs://weatherbench2/datasets/hres_t0/2016-2022-6h-1440x721.zarr\"\n",
    "ds = xr.open_zarr(fsspec.get_mapper(url), chunks=None)\n",
    "\n",
    "time = '2022-05-11'\n",
    "\n",
    "# Download the surface-level variables.\n",
    "if not (download_path / f\"{time}-surface-level.nc\").exists():\n",
    "    surface_vars = [\"10m_u_component_of_wind\", \"10m_v_component_of_wind\", \"2m_temperature\", \"mean_sea_level_pressure\"]\n",
    "    ds_surf = ds[surface_vars].sel(time=time).compute()\n",
    "    ds_surf.to_netcdf(str(download_path / f\"{time}-surface-level.nc\"))\n",
    "print(\"Surface-level variables downloaded!\")\n",
    "\n",
    "# Download the atmospheric variables.\n",
    "if not (download_path / f\"{time}-atmospheric.nc\").exists():\n",
    "    atmos_vars = [\"temperature\", \"u_component_of_wind\", \"v_component_of_wind\", \"specific_humidity\", \"geopotential\"]\n",
    "    ds_atmos = ds[atmos_vars].sel(time=time).compute()\n",
    "    ds_atmos.to_netcdf(str(download_path / f\"{time}-atmospheric.nc\"))\n",
    "print(\"Atmos-level variables downloaded!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d166a5f",
   "metadata": {},
   "source": [
    "## Downloading Static Variables from ERA5 Data\n",
    "The static variables are not available in WeatherBench2, so we need to download them from ERA5 as in `example.ipynb`. To do so, register an account with the [Climate Data Store](https://cds.climate.copernicus.eu/) and create `$HOME/.cdsapirc` with the following content:\n",
    "\n",
    "```\n",
    "url: https://cds.climate.copernicus.eu/api/v2\n",
    "key: <UID>:<API key>\n",
    "```\n",
    "\n",
    "You can find your UID and API key on your account page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0541bf19-024f-4c76-8666-9d559640156e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Static variables downloaded!\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import cdsapi\n",
    "\n",
    "# Data will be downloaded here.\n",
    "download_path = Path(\"~/downloads\")\n",
    "\n",
    "c = cdsapi.Client()\n",
    "\n",
    "download_path = download_path.expanduser()\n",
    "download_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Download the static variables.\n",
    "if not (download_path / \"static.nc\").exists():\n",
    "    c.retrieve(\n",
    "        \"reanalysis-era5-single-levels\",\n",
    "        {\n",
    "            \"product_type\": \"reanalysis\",\n",
    "            \"variable\": [\n",
    "                \"geopotential\",\n",
    "                \"land_sea_mask\",\n",
    "                \"soil_type\",\n",
    "            ],\n",
    "            \"year\": \"2023\",\n",
    "            \"month\": \"01\",\n",
    "            \"day\": \"01\",\n",
    "            \"time\": \"00:00\",\n",
    "            \"format\": \"netcdf\",\n",
    "        },\n",
    "        str(download_path / \"static.nc\"),\n",
    "    )\n",
    "print(\"Static variables downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7d8aa4-a16b-481c-b7e5-66764d6e98f1",
   "metadata": {},
   "source": [
    "## Preparing a Batch\n",
    "\n",
    "We convert the downloaded data to an `aurora.Batch`, which is what the model requires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fba4fb22-475b-4156-94c8-e61c77a20539",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/t-analucic/mambaforge/envs/climai_global/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import xarray as xr\n",
    "\n",
    "from aurora import Batch, Metadata\n",
    "\n",
    "static_vars_ds = xr.open_dataset(download_path / \"static.nc\", engine=\"netcdf4\")\n",
    "surf_vars_ds = xr.open_dataset(download_path / f\"{time}-surface-level.nc\", engine=\"netcdf4\")\n",
    "atmos_vars_ds = xr.open_dataset(download_path / f\"{time}-atmospheric.nc\", engine=\"netcdf4\")\n",
    "\n",
    "i = 1  # Select this time index in the downloaded data.\n",
    "\n",
    "batch = Batch(\n",
    "    surf_vars={\n",
    "        # First select time points `i` and `i - 1`. Afterwards, `[None]` inserts a\n",
    "        # batch dimension of size one.\n",
    "        \"2t\": torch.from_numpy(surf_vars_ds[\"2m_temperature\"].values[[i - 1, i]][None]),\n",
    "        \"10u\": torch.from_numpy(surf_vars_ds[\"10m_u_component_of_wind\"].values[[i - 1, i]][None]),\n",
    "        \"10v\": torch.from_numpy(surf_vars_ds[\"10m_v_component_of_wind\"].values[[i - 1, i]][None]),\n",
    "        \"msl\": torch.from_numpy(surf_vars_ds[\"mean_sea_level_pressure\"].values[[i - 1, i]][None]),\n",
    "    },\n",
    "    static_vars={\n",
    "        # The static variables are constant, so we just get them for the first time.\n",
    "        \"z\": torch.from_numpy(static_vars_ds[\"z\"].values[0]),\n",
    "        \"slt\": torch.from_numpy(static_vars_ds[\"slt\"].values[0]),\n",
    "        \"lsm\": torch.from_numpy(static_vars_ds[\"lsm\"].values[0]),\n",
    "    },\n",
    "    atmos_vars={\n",
    "        \"t\": torch.from_numpy(atmos_vars_ds[\"temperature\"].values[[i - 1, i]][None]),\n",
    "        \"u\": torch.from_numpy(atmos_vars_ds[\"u_component_of_wind\"].values[[i - 1, i]][None]),\n",
    "        \"v\": torch.from_numpy(atmos_vars_ds[\"v_component_of_wind\"].values[[i - 1, i]][None]),\n",
    "        \"q\": torch.from_numpy(atmos_vars_ds[\"specific_humidity\"].values[[i - 1, i]][None]),\n",
    "        \"z\": torch.from_numpy(atmos_vars_ds[\"geopotential\"].values[[i - 1, i]][None]),\n",
    "    },\n",
    "    metadata=Metadata(\n",
    "        lat=torch.from_numpy(surf_vars_ds.latitude.values * -1),    # Metadata expects latitudes to be in the range [90, -90].\n",
    "        lon=torch.from_numpy(surf_vars_ds.longitude.values),\n",
    "        # Converting to `datetime64[s]` ensures that the output of `tolist()` gives\n",
    "        # `datetime.datetime`s. Note that this needs to be a tuple of length one:\n",
    "        # one value for every batch element.\n",
    "        time=(surf_vars_ds.time.values.astype(\"datetime64[s]\").tolist()[i],),\n",
    "        atmos_levels=tuple(int(level) for level in atmos_vars_ds.level.values),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6f12fe-1e49-49f0-bf03-bffcea9d1551",
   "metadata": {},
   "source": [
    "## Loading and Running the Model\n",
    "\n",
    "Finally, we are ready to load and run the model and visualise the predictions. We perform a roll-out for two steps, which produces predictions for hours 12:00 and 18:00."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4824be34-060d-422a-addf-841c2c3609b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aurora-0.25-finetuned.ckpt: 100%|██████████| 5.04G/5.04G [02:04<00:00, 40.6MB/s]\n",
      "/home/t-analucic/mambaforge/envs/climai_global/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "ename": "type",
     "evalue": "CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[0;32m---> 10\u001b[0m     preds \u001b[38;5;241m=\u001b[39m [pred\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m rollout(model, batch, steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m     12\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[0;32m---> 10\u001b[0m     preds \u001b[38;5;241m=\u001b[39m [pred\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m rollout(model, batch, steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m     12\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/aurora/aurora/rollout.py:33\u001b[0m, in \u001b[0;36mrollout\u001b[0;34m(model, batch, steps)\u001b[0m\n\u001b[1;32m     30\u001b[0m batch \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mto(p\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(steps):\n\u001b[0;32m---> 33\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m pred\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# Add the appropriate history so the model can be run on the prediction.\u001b[39;00m\n",
      "File \u001b[0;32m~/aurora/aurora/model/aurora.py:173\u001b[0m, in \u001b[0;36mAurora.forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    164\u001b[0m batch \u001b[38;5;241m=\u001b[39m dataclasses\u001b[38;5;241m.\u001b[39mreplace(\n\u001b[1;32m    165\u001b[0m     batch,\n\u001b[1;32m    166\u001b[0m     static_vars\u001b[38;5;241m=\u001b[39m{k: v[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]\u001b[38;5;241m.\u001b[39mrepeat(B, T, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mstatic_vars\u001b[38;5;241m.\u001b[39mitems()},\n\u001b[1;32m    167\u001b[0m )\n\u001b[1;32m    169\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m    170\u001b[0m     batch,\n\u001b[1;32m    171\u001b[0m     lead_time\u001b[38;5;241m=\u001b[39mtimedelta(hours\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m),\n\u001b[1;32m    172\u001b[0m )\n\u001b[0;32m--> 173\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlead_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimedelta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhours\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatch_res\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatch_res\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrollout_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(\n\u001b[1;32m    180\u001b[0m     x,\n\u001b[1;32m    181\u001b[0m     batch,\n\u001b[1;32m    182\u001b[0m     lead_time\u001b[38;5;241m=\u001b[39mtimedelta(hours\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m),\n\u001b[1;32m    183\u001b[0m     patch_res\u001b[38;5;241m=\u001b[39mpatch_res,\n\u001b[1;32m    184\u001b[0m )\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# Remove batch and history dimension from static variables.\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/climai_global/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/aurora/aurora/model/swin3d.py:917\u001b[0m, in \u001b[0;36mSwin3DTransformerBackbone.forward\u001b[0;34m(self, x, lead_time, rollout_step, patch_res)\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_layers):\n\u001b[1;32m    916\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_decoder_layers \u001b[38;5;241m-\u001b[39m i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 917\u001b[0m     x, _ \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mall_enc_res\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadded_outs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrollout_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrollout_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_decoder_layers \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# For the intermediate stages, we use additive skip connections.\u001b[39;00m\n\u001b[1;32m    927\u001b[0m         x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m skips[index \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/mambaforge/envs/climai_global/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/aurora/aurora/model/swin3d.py:722\u001b[0m, in \u001b[0;36mBasicLayer3D.forward\u001b[0;34m(self, x, c, res, crop, rollout_step)\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run the basic layer.\u001b[39;00m\n\u001b[1;32m    710\u001b[0m \n\u001b[1;32m    711\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;124;03m    torch.Tensor: Output tokens.\u001b[39;00m\n\u001b[1;32m    720\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m--> 722\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrollout_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    724\u001b[0m     x_scaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample(x, res)\n",
      "File \u001b[0;32m~/mambaforge/envs/climai_global/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/aurora/aurora/model/swin3d.py:486\u001b[0m, in \u001b[0;36mSwin3DTransformerBlock.forward\u001b[0;34m(self, x, c, res, rollout_step, warped)\u001b[0m\n\u001b[1;32m    483\u001b[0m x_windows \u001b[38;5;241m=\u001b[39m x_windows\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, ws[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m ws[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m ws[\u001b[38;5;241m2\u001b[39m], D)  \u001b[38;5;66;03m# (nW*B, ws*ws, D)\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;66;03m# W-MSA/SW-MSA. Has shape (nW*B, ws*ws, D).\u001b[39;00m\n\u001b[0;32m--> 486\u001b[0m attn_windows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_windows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrollout_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrollout_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# Merge the windows into the original input (patch) resolution.\u001b[39;00m\n\u001b[1;32m    489\u001b[0m attn_windows \u001b[38;5;241m=\u001b[39m attn_windows\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, ws[\u001b[38;5;241m0\u001b[39m], ws[\u001b[38;5;241m1\u001b[39m], ws[\u001b[38;5;241m2\u001b[39m], D)  \u001b[38;5;66;03m# (nW*B, Wc, Wh, Ww, D)\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/climai_global/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/aurora/aurora/model/swin3d.py:152\u001b[0m, in \u001b[0;36mWindowAttention.forward\u001b[0;34m(self, x, mask, rollout_step)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    137\u001b[0m     x: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    138\u001b[0m     mask: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    139\u001b[0m     rollout_step: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    140\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    141\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Run the forward pass of the window-based multi-head self-attention layer.\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;124;03m        torch.Tensor: Output of shape `(nW*B, N, C)`.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m     qkv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqkv(x) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlora_qkv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrollout_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m     qkv \u001b[38;5;241m=\u001b[39m rearrange(qkv, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB N (qkv H D) -> qkv B H N D\u001b[39m\u001b[38;5;124m\"\u001b[39m, H\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, qkv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m    154\u001b[0m     q, k, v \u001b[38;5;241m=\u001b[39m qkv[\u001b[38;5;241m0\u001b[39m], qkv[\u001b[38;5;241m1\u001b[39m], qkv[\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[0;32m~/mambaforge/envs/climai_global/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/aurora/aurora/model/lora.py:119\u001b[0m, in \u001b[0;36mLoRARollout.forward\u001b[0;34m(self, x, step)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloras\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloras[step](x)\n",
      "File \u001b[0;32m~/mambaforge/envs/climai_global/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/aurora/aurora/model/lora.py:62\u001b[0m, in \u001b[0;36mLoRA.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     54\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the LoRA adaptation.\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m        torch.Tensor: Additive correction for the output of the linear layer.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlora_dropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlora_A\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_B\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaling\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`"
     ]
    }
   ],
   "source": [
    "from aurora import Aurora, rollout\n",
    "\n",
    "model = Aurora(use_lora=True)  # The 0.25 fine-tuned version uses LoRA.\n",
    "model.load_checkpoint(\"wbruinsma/aurora\", \"aurora-0.25-finetuned.ckpt\")\n",
    "\n",
    "model.eval()\n",
    "model = model.to(\"cuda\")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    preds = [pred.to(\"cpu\") for pred in rollout(model, batch, steps=1)]\n",
    "\n",
    "model = model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e84170-5802-46ad-b4d8-6eb9bf8c98ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(12, 6.5))\n",
    "\n",
    "for i in range(ax.shape[0]):\n",
    "    pred = preds[i]\n",
    "    \n",
    "    ax[i, 0].imshow(pred.surf_vars[\"2t\"][0, 0].numpy() - 273.15, vmin=-50, vmax=50)\n",
    "    ax[i, 0].set_ylabel(str(pred.metadata.time[0]))\n",
    "    if i == 0:\n",
    "        ax[i, 0].set_title(\"Aurora Prediction\")\n",
    "    ax[i, 0].set_xticks([])\n",
    "    ax[i, 0].set_yticks([])\n",
    "    \n",
    "    ax[i, 1].imshow(surf_vars_ds[\"t2m\"][2 + i].values - 273.15, vmin=-50, vmax=50)\n",
    "    if i == 0:\n",
    "        ax[i, 1].set_title(\"ERA5\")\n",
    "    ax[i, 1].set_xticks([])\n",
    "    ax[i, 1].set_yticks([])\n",
    "\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
